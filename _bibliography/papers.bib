---
---
@INPROCEEDINGS{10744527,
  abbr={IJCB2024},
  author={Guo, Yuxiang and Huang, Siyuan and Prabhakar, Ram and Lau, Chun Pong and Chellappa, Rama and Peng, Cheng},
  abstract={Gait recognition holds the promise of robustly identifying subjects based on walking patterns instead of appearance information. While previous approaches have performed well for curated indoor data, they tend to underperform in unconstrained situations, e.g. in outdoor, long distance scenes, etc. We propose a framework, termed GAit DEtection and Recognition (GADER), for human authentication in challenging outdoor scenarios. Specifically, GADER leverages a Double Helical Signature to detect segments that contain human movement and builds discriminative features through a novel gait recognition method, where only frames containing gait information are used. To further enhance robustness, GADER encodes viewpoint information in its architecture, and distills representation from an auxiliary RGB recognition model, which enables GADER to learn from silhouette and RGB data at training time. At test time, GADER only infers from the silhouette modality. We evaluate our method on multiple State-of-The-Arts(SoTA) gait baselines and demonstrate consistent improvements on indoor and outdoor datasets, especially with a significant 25.2% improvement on unconstrained, remote gait data.},
  booktitle={2024 IEEE International Joint Conference on Biometrics (IJCB)}, 
  title={Distillation-guided Representation Learning for Unconstrained Gait Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={1-11},
  pdf={Distillation-guided_Representation_Learning_for_Unconstrained_Gait_Recognition.pdf},
  keywords={Training;Representation learning;Legged locomotion;Pipelines;Detectors;Information leakage;Feature extraction;Robustness;Gait recognition;Standards},
  bibtex_show={true},
  selected={true},
  preview={IJCB2024.jpg},
  html={https://ieeexplore.ieee.org/abstract/document/10744527},
  award={<em><a href="https://iapr-tc4.org/blog/2024/09/21/2024-iapr-bbspa-winner-yuxiang-guo/">IAPR Best Biometrics Student Paper Award</a>},
  doi={10.1109/IJCB62174.2024.10744527}}

@INPROCEEDINGS{10042572,
  abbr={FG2023},
  author={Guo*, Yuxiang and Peng*, Cheng and Lau, Chun Pong and Chellappa, Rama},
  abstract={Whole-body-based human authentication is a promising approach for remote biometrics scenarios. Current literature focuses on either body recognition based on RGB images or gait recognition based on body shapes and walking patterns; both have their advantages and drawbacks. In this work, we propose Dual-Modal Ensemble (DME), which combines both RGB and silhouette data to achieve more robust performances for indoor and outdoor whole-body based recognition. Within DME, we propose GaitPattern, which is inspired by the double helical gait pattern used in traditional gait analysis. The GaitPattern contributes to robust identification performance over a large range of viewing angles. Extensive experimental results on the CASIA-B dataset demonstrate that the proposed method outperforms state-of-the-art recognition systems. We also provide experimental results using the newly collected BRIAR dataset.},
  booktitle={2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)}, 
  title={Multi-Modal Human Authentication Using Silhouettes, Gait and RGB}, 
  annotation={* represents equal contribution},
  year={2023},
  volume={},
  number={},
  pages={1-7},
  keywords={Training;Legged locomotion;Image recognition;Shape;Face recognition;Authentication;Gesture recognition},
  html={https://ieeexplore.ieee.org/abstract/document/10042572},
  pdf={Multi-Modal_Human_Authentication_Using_Silhouettes_Gait_and_RGB.pdf},
  selected={false},
  preview={fg2023.jpg},
  doi={10.1109/FG57933.2023.10042572},
  bibtex_show={true}}

@article{guo2023gaitcontour,
  abbr={WACV2025},
  title={GaitContour: Efficient Gait Recognition based on a Contour-Pose Representation},
  abstract={Gait recognition holds the promise to robustly identify subjects based on walking patterns instead of appearance information. In recent years, this field has been dominated by learning methods based on two principal input representations: dense silhouette masks or sparse pose keypoints. In this work, we propose a novel, point-based Contour-Pose representation, which compactly expresses both body shape and body parts information. We further propose a local-to-global architecture, called GaitContour, to leverage this novel representation and efficiently compute subject embedding in two stages. The first stage consists of a local transformer that extracts features from five different body regions. The second stage then aggregates the regional features to estimate a global human gait representation. Such a design significantly reduces the complexity of the attention operation and improves efficiency and performance simultaneously. Through large scale experiments, GaitContour is shown to perform significantly better than previous point-based methods, while also being significantly more efficient than silhouette-based methods. On challenging datasets with significant distractors, GaitContour can even outperform silhouette-based methods.},
  author={Guo, Yuxiang and Shah, Anshul and Liu, Jiang and Gupta, Ayush and Chellappa, Rama and Peng, Cheng},
  journal={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year={2025},
  html={https://arxiv.org/abs/2311.16497},
  booktitle={2025 Winter Conference on Applications of Computer Vision (WACV)}, 
  pdf={gaitcontour.pdf},
  preview={GaitContour.png},
  bibtex_show={true},
  selected={true},
}

@article{guo2024stimuvar,
  abbr={IJCV},
  title={Stimuvar: Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models},
  author={Guo, Yuxiang and Siddiqui, Faizan and Zhao, Yang and Chellappa, Rama and Lo, Shao-Yuan},
  abstract={Predicting and reasoning how a video would make a human feel is crucial for developing socially intelligent systems. Although Multimodal Large Language Models (MLLMs) have shown impressive video understanding capabilities, they tend to focus more on the semantic content of videos, often overlooking emotional stimuli. Hence, most existing MLLMs fall short in estimating viewers' emotional reactions and providing plausible explanations. To address this issue, we propose StimuVAR, a spatiotemporal Stimuli-aware framework for Video Affective Reasoning (VAR) with MLLMs. StimuVAR incorporates a two-level stimuli-aware mechanism: frame-level awareness and token-level awareness. Frame-level awareness involves sampling video frames with events that are most likely to evoke viewers' emotions. Token-level awareness performs tube selection in the token space to make the MLLM concentrate on emotion-triggered spatiotemporal regions. Furthermore, we create VAR instruction data to perform affective training, steering MLLMs' reasoning strengths towards emotional focus and thereby enhancing their affective reasoning ability. To thoroughly assess the effectiveness of VAR, we provide a comprehensive evaluation protocol with extensive metrics. StimuVAR is the first MLLM-based method for viewer-centered VAR. Experiments demonstrate its superiority in understanding viewers' emotional responses to videos and providing coherent and insightful explanations.},
  journal={International Journal of Computer Vision},
  year={2024},
  html={https://arxiv.org/abs/2311.16497},
  pdf={StimuVAR.pdf},
  preview={Stimuvar.png},
  bibtex_show={true},
  selected={true},
  additional_info={Posted by <a href="https://engineering.jhu.edu/ece/news/ai-with-a-heart-tech-that-knows-what-humans-are-feeling/">JHU Whiting School</a>},
}

@article{tang2024spars3r,
  abbr={CVPR2025},
  title={SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D Reconstruction},
  author={Tang*, Yutao and Guo*, Yuxiang and Li, Deming and Peng, Cheng},
  journal={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025},
  abstract={Recent efforts in Gaussian-Splat-based Novel View Synthesis can achieve photorealistic rendering; however, such capability is limited in sparse-view scenarios due to sparse initialization and over-fitting floaters. Recent progress in depth estimation and alignment can provide dense point cloud with few views; however, the resulting pose accuracy is suboptimal. In this work, we present SPARS3R, which combines the advantages of accurate pose estimation from Structure-from-Motion and dense point cloud from depth estimation. To this end, SPARS3R first performs a Global Fusion Alignment process that maps a prior dense point cloud to a sparse point cloud from Structure-from-Motion based on triangulated correspondences. RANSAC is applied during this process to distinguish inliers and outliers. SPARS3R then performs a second, Semantic Outlier Alignment step, which extracts semantically coherent regions around the outliers and performs local alignment in these regions. Along with several improvements in the evaluation process, we demonstrate that SPARS3R can achieve photorealistic rendering with sparse images and significantly outperforms existing approaches.},
  html={https://arxiv.org/abs/2411.12592},
  pdf={Spars3r.pdf},
  preview={spars3r.png},
  bibtex_show={true},
  selected={true},
  annotation={* represents equal contribution},
}

@artical{Guo2025ImageDoctorDT,
  title={ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image Reasoning},
  author={Guo*, Yuxiang and Liu*, Jiang and Wang, Ze and Chen, Hao and Sun, Ximeng and Zhao, Yang and Wu, Jialian and Yu, Xiaodong and Liu, Zicheng and Barsoum, Emad},
  year={2025},
  abstract={The rapid advancement of text-to-image (T2I) models has increased the need for reliable human preference modeling, a demand further amplified by recent progress in reinforcement learning for preference alignment. However, existing approaches typically quantify the quality of a generated image using a single scalar, limiting their ability to provide comprehensive and interpretable feedback on image quality. To address this, we introduce ImageDoctor, a unified multi-aspect T2I model evaluation framework that assesses image quality across four complementary dimensions: plausibility, semantic alignment, aesthetics, and overall quality. ImageDoctor also provides pixel-level flaw indicators in the form of heatmaps, which highlight misaligned or implausible regions, and can be used as a dense reward for T2I model preference alignment. Inspired by the diagnostic process, we improve the detail sensitivity and reasoning capability of ImageDoctor by introducing a “look-think-predict” paradigm, where the model first localizes potential flaws, then generates reasoning, and finally concludes the evaluation with quantitative scores. Built on top of a vision-language model and trained through a combination of supervised fine-tuning and reinforcement learning, ImageDoctor demonstrates strong alignment with human preference across multiple datasets, establishing its effectiveness as an evaluation metric. Furthermore, when used as a reward model for preference tuning, ImageDoctor significantly improves generation quality—achieving an improvement of 10% over scalar-based reward models.}
  html={https://arxiv.org/abs/2510.01010v1},
  annotation={* represents equal contribution},
  bibtex_show={true},
  selected={true},
  pdf={ImageDoctor.pdf},
  preview={Icon.png},
  website={https://image-doctor.github.io/}
}

@article{Huang2025WholeBodyDI,
  abbr={T-BIOM},
  title={Whole-Body Detection, Identification and Recognition at Altitude and Range},
  author={Huang, Siyuan and Kathirvel, Ram Prabhakar  and Guo, Yuxiang  and Lau, Chun Pong  and Chellappa, Rama },
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
  year={2025},
  volume={7},
  pages={331-343},
  html={https://ieeexplore.ieee.org/abstract/document/10737205},
  bibtex_show={true},
  pdf={Whole-Body_Detection_Identification_and_Recognition_at_Altitude_and_Range.pdf},
}

@article{Guo2025DistillationGuidedRL,
  abbr={T-BIOM},
  title={Distillation-Guided Representation Learning for Unconstrained Video Human Authentication},
  author={Guo, Yuxiang and Huang, Siyuan and Kathirvel, Ram Prabhakar and Lau, Chun Pong and Chellappa, Ramalingam and Peng, Cheng},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
  year={2025},
  volume={7},
  pages={940-952},
  html={https://ieeexplore.ieee.org/document/11111687},
  bibtex_show={true},
  pdf={Distillation-Guided_Representation_Learning_for_Unconstrained_Video_Human_Authentication.pdf}
}

@InProceedings{Huang_2025_WACV,
    abbr={WACV2025},
    author    = {Huang, Siyuan and Kathirvel, Ram Prabhakar and Guo, Yuxiang and Chellappa, Rama and Peng, Cheng},
    title     = {VILLS : Video-Image Learning to Learn Semantics for Person Re-Identification},
    journal   ={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {February},
    year      = {2025},
    pages     = {5969-5979},
    bibtex_show={true},
    html={https://openaccess.thecvf.com/content/WACV2025/html/Guo_GaitContour_Efficient_Gait_Recognition_Based_on_a_Contour-Pose_Representation_WACV_2025_paper.html},
}

@article{liu2023instruct2attack,
  title={Instruct2attack: Language-guided semantic adversarial attacks},
  author={Liu, Jiang and Wei, Chen and Guo, Yuxiang and Yu, Heng and Yuille, Alan and Feizi, Soheil and Lau, Chun Pong and Chellappa, Rama},
  journal={arXiv preprint arXiv:2311.15551},
  year={2023}
}

@article{liu2024immersive,
  title={ULTRA-360: Unconstrained Dataset for Large-scale Temporal 3D Reconstruction across Altitudes and Omnidirectional Views},
  author={Liu*, Xijun and Zhang*, Zhaoliang and Guo* Yuxiang and Zhou, Yifan and Chellappa, Rama and Peng, Cheng},
  journal={arXiv preprint arXiv:2412.14418},
  year={2024}
}

@article{wang2025evoworld,
  title={EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory},
  author={Wang, Jiahao and Ye, Luoxin and ... and Guo, Yuxiang and Liu, Xijun and Chellappa, Rama and Peng, Cheng and Yuille, Alan and others},
  journal={arXiv preprint arXiv:2510.01183},
  year={2025}
}

@INPROCEEDINGS{11099490,
  author={Wang, Zhao-Yang and Liu, Jiang and Guo, Yuxiang and Chen, Jieneng and Chellappa, Rama},
  booktitle={2025 IEEE 19th International Conference on Automatic Face and Gesture Recognition (FG)}, 
  title={UniGait: A Unified Transformer-based Multitask Framework for Gait Analysis in the Wild}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  keywords={Meters;Accuracy;Face recognition;Estimation;Medical services;Feature extraction;Transformers;Older adults;Gait recognition;Monitoring},
  doi={10.1109/FG61629.2025.11099490}}
