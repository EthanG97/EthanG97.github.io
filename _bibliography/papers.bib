---
---
@INPROCEEDINGS{10744527,
  abbr={IJCB2024},
  author={Guo, Yuxiang and Huang, Siyuan and Prabhakar, Ram and Lau, Chun Pong and Chellappa, Rama and Peng, Cheng},
  abstract={Gait recognition holds the promise of robustly identifying subjects based on walking patterns instead of appearance information. While previous approaches have performed well for curated indoor data, they tend to underperform in unconstrained situations, e.g. in outdoor, long distance scenes, etc. We propose a framework, termed GAit DEtection and Recognition (GADER), for human authentication in challenging outdoor scenarios. Specifically, GADER leverages a Double Helical Signature to detect segments that contain human movement and builds discriminative features through a novel gait recognition method, where only frames containing gait information are used. To further enhance robustness, GADER encodes viewpoint information in its architecture, and distills representation from an auxiliary RGB recognition model, which enables GADER to learn from silhouette and RGB data at training time. At test time, GADER only infers from the silhouette modality. We evaluate our method on multiple State-of-The-Arts(SoTA) gait baselines and demonstrate consistent improvements on indoor and outdoor datasets, especially with a significant 25.2% improvement on unconstrained, remote gait data.},
  booktitle={2024 IEEE International Joint Conference on Biometrics (IJCB)}, 
  title={Distillation-guided Representation Learning for Unconstrained Gait Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={1-11},
  pdf={Distillation-guided_Representation_Learning_for_Unconstrained_Gait_Recognition.pdf},
  keywords={Training;Representation learning;Legged locomotion;Pipelines;Detectors;Information leakage;Feature extraction;Robustness;Gait recognition;Standards},
  bibtex_show={true},
  selected={true},
  preview={IJCB2024.jpg},
  html={https://ieeexplore.ieee.org/abstract/document/10744527},
  award={<em><a href="https://iapr-tc4.org/blog/2024/09/21/2024-iapr-bbspa-winner-yuxiang-guo/">IAPR Best Biometrics Student Paper Award</a>},
  doi={10.1109/IJCB62174.2024.10744527}}

@INPROCEEDINGS{10042572,
  abbr={FG2023},
  author={Guo*, Yuxiang and Peng*, Cheng and Lau, Chun Pong and Chellappa, Rama},
  abstract={Whole-body-based human authentication is a promising approach for remote biometrics scenarios. Current literature focuses on either body recognition based on RGB images or gait recognition based on body shapes and walking patterns; both have their advantages and drawbacks. In this work, we propose Dual-Modal Ensemble (DME), which combines both RGB and silhouette data to achieve more robust performances for indoor and outdoor whole-body based recognition. Within DME, we propose GaitPattern, which is inspired by the double helical gait pattern used in traditional gait analysis. The GaitPattern contributes to robust identification performance over a large range of viewing angles. Extensive experimental results on the CASIA-B dataset demonstrate that the proposed method outperforms state-of-the-art recognition systems. We also provide experimental results using the newly collected BRIAR dataset.},
  booktitle={2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)}, 
  title={Multi-Modal Human Authentication Using Silhouettes, Gait and RGB}, 
  annotation={* represents equal contribution},
  year={2023},
  volume={},
  number={},
  pages={1-7},
  keywords={Training;Legged locomotion;Image recognition;Shape;Face recognition;Authentication;Gesture recognition},
  html={https://ieeexplore.ieee.org/abstract/document/10042572},
  pdf={Multi-Modal_Human_Authentication_Using_Silhouettes_Gait_and_RGB.pdf},
  selected={true},
  preview={fg2023.jpg},
  doi={10.1109/FG57933.2023.10042572},
  award={<em><a href="https://iros2022.org/2022/10/30/award-winners/">Best Paper Award finalist</a>, <p style="color: red; display: inline;">(0.6%)</p></em>},
  bibtex_show={true}}

@article{guo2023gaitcontour,
  abbr={WACV2025},
  title={GaitContour: Efficient Gait Recognition based on a Contour-Pose Representation},
  abstract={Gait recognition holds the promise to robustly identify subjects based on walking patterns instead of appearance information. In recent years, this field has been dominated by learning methods based on two principal input representations: dense silhouette masks or sparse pose keypoints. In this work, we propose a novel, point-based Contour-Pose representation, which compactly expresses both body shape and body parts information. We further propose a local-to-global architecture, called GaitContour, to leverage this novel representation and efficiently compute subject embedding in two stages. The first stage consists of a local transformer that extracts features from five different body regions. The second stage then aggregates the regional features to estimate a global human gait representation. Such a design significantly reduces the complexity of the attention operation and improves efficiency and performance simultaneously. Through large scale experiments, GaitContour is shown to perform significantly better than previous point-based methods, while also being significantly more efficient than silhouette-based methods. On challenging datasets with significant distractors, GaitContour can even outperform silhouette-based methods.},
  author={Guo, Yuxiang and Shah, Anshul and Liu, Jiang and Gupta, Ayush and Chellappa, Rama and Peng, Cheng},
  journal={arXiv preprint arXiv:2311.16497},
  year={2025},
  html={https://arxiv.org/abs/2311.16497},
  pdf={gaitcontour.pdf},
  preview={GaitContour.png},
  bibtex_show={true},
  selected={true},
}

@article{guo2024stimuvar,
  title={Stimuvar: Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models},
  author={Guo, Yuxiang and Siddiqui, Faizan and Zhao, Yang and Chellappa, Rama and Lo, Shao-Yuan},
  abstract={Predicting and reasoning how a video would make a human feel is crucial for developing socially intelligent systems. Although Multimodal Large Language Models (MLLMs) have shown impressive video understanding capabilities, they tend to focus more on the semantic content of videos, often overlooking emotional stimuli. Hence, most existing MLLMs fall short in estimating viewers' emotional reactions and providing plausible explanations. To address this issue, we propose StimuVAR, a spatiotemporal Stimuli-aware framework for Video Affective Reasoning (VAR) with MLLMs. StimuVAR incorporates a two-level stimuli-aware mechanism: frame-level awareness and token-level awareness. Frame-level awareness involves sampling video frames with events that are most likely to evoke viewers' emotions. Token-level awareness performs tube selection in the token space to make the MLLM concentrate on emotion-triggered spatiotemporal regions. Furthermore, we create VAR instruction data to perform affective training, steering MLLMs' reasoning strengths towards emotional focus and thereby enhancing their affective reasoning ability. To thoroughly assess the effectiveness of VAR, we provide a comprehensive evaluation protocol with extensive metrics. StimuVAR is the first MLLM-based method for viewer-centered VAR. Experiments demonstrate its superiority in understanding viewers' emotional responses to videos and providing coherent and insightful explanations.},
  journal={arXiv preprint arXiv:2409.00304},
  year={2024},
  html={https://arxiv.org/abs/2311.16497},
  pdf={StimuVAR.pdf},
  preview={Stimuvar.png},
  bibtex_show={true},
  selected={true},
  additional_info={Posted by <a href="https://engineering.jhu.edu/ece/news/ai-with-a-heart-tech-that-knows-what-humans-are-feeling/">JHU Whiting School</a>},
}

@article{tang2024spars3r,
  title={SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D Reconstruction},
  author={Tang*, Yutao and Guo*, Yuxiang and Li, Deming and Peng, Cheng},
  journal={arXiv preprint arXiv:2411.12592},
  year={2024},
  abstract={Recent efforts in Gaussian-Splat-based Novel View Synthesis can achieve photorealistic rendering; however, such capability is limited in sparse-view scenarios due to sparse initialization and over-fitting floaters. Recent progress in depth estimation and alignment can provide dense point cloud with few views; however, the resulting pose accuracy is suboptimal. In this work, we present SPARS3R, which combines the advantages of accurate pose estimation from Structure-from-Motion and dense point cloud from depth estimation. To this end, SPARS3R first performs a Global Fusion Alignment process that maps a prior dense point cloud to a sparse point cloud from Structure-from-Motion based on triangulated correspondences. RANSAC is applied during this process to distinguish inliers and outliers. SPARS3R then performs a second, Semantic Outlier Alignment step, which extracts semantically coherent regions around the outliers and performs local alignment in these regions. Along with several improvements in the evaluation process, we demonstrate that SPARS3R can achieve photorealistic rendering with sparse images and significantly outperforms existing approaches.},
  html={https://arxiv.org/abs/2411.12592},
  pdf={Spars3r.pdf},
  preview={spars3r.png},
  bibtex_show={true},
  selected={true},
}
